{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07221968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:12:28.368506Z",
     "iopub.status.busy": "2022-07-05T14:12:28.367013Z",
     "iopub.status.idle": "2022-07-05T14:12:58.045995Z",
     "shell.execute_reply": "2022-07-05T14:12:58.045335Z",
     "shell.execute_reply.started": "2022-07-05T14:02:48.192904Z"
    },
    "papermill": {
     "duration": 29.718779,
     "end_time": "2022-07-05T14:12:58.046168",
     "exception": false,
     "start_time": "2022-07-05T14:12:28.327389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b55b42",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-05T14:12:58.129130Z",
     "iopub.status.busy": "2022-07-05T14:12:58.128530Z",
     "iopub.status.idle": "2022-07-05T14:13:11.771937Z",
     "shell.execute_reply": "2022-07-05T14:13:11.770988Z",
     "shell.execute_reply.started": "2022-07-05T14:03:20.827021Z"
    },
    "papermill": {
     "duration": 13.690109,
     "end_time": "2022-07-05T14:13:11.772092",
     "exception": false,
     "start_time": "2022-07-05T14:12:58.081983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# For Transformer Models\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, AutoConfig\n",
    "\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.metrics import pairwise_distances\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    seed = 42 # 随机种子\n",
    "    n_candidates = 50 # 候选集数量\n",
    "    n_spatial_candidates = 10 # 空间候选集数量\n",
    "    n_splits = 4 # 折数\n",
    "    max_length = 64 # 最大长度\n",
    "\n",
    "    device = torch.device('cuda') # GPU\n",
    "\n",
    "    # Arcface\n",
    "    s = 30.0 # arcface 参数 scale\n",
    "    m = 0.5  # arcface 参数 margin\n",
    "    ls_eps = 0.0 # arcface 参数 eps\n",
    "    easy_margin = False # easy_margin\n",
    "    n_classes = 739972 # 分类数量\n",
    "\n",
    "    \n",
    "# 四个模型\n",
    "class CFG_Model1:\n",
    "    model_name = \"../input/huggingface-roberta-variants/xlm-roberta-large/xlm-roberta-large\" \n",
    "    weight = \"../input/model1/xlm-roberta-large_epoch30.bin\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class CFG_Model2:\n",
    "    model_name = \"../input/sentence-transformers/LaBSE/0_Transformer\"\n",
    "    weight = \"../input/model2/0_Transformer_epoch30.bin\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "class CFG_Model3:\n",
    "    model_name = \"../input/paraphrasemultilingualmpnetbasev2/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    weight = \"../input/model3/paraphrase-multilingual-mpnet-base-v2_epoch30.bin\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "class CFG_Model4:\n",
    "    model_name = \"../input/rembert-pt\"\n",
    "    weight = \"../input/model4/rembert-pt_epoch30.bin\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f78fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:11.916074Z",
     "iopub.status.busy": "2022-07-05T14:13:11.915370Z",
     "iopub.status.idle": "2022-07-05T14:13:22.315136Z",
     "shell.execute_reply": "2022-07-05T14:13:22.314287Z",
     "shell.execute_reply.started": "2022-07-05T14:03:40.959149Z"
    },
    "papermill": {
     "duration": 10.441877,
     "end_time": "2022-07-05T14:13:22.315283",
     "exception": false,
     "start_time": "2022-07-05T14:13:11.873406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test.csv\n",
    "df = cudf.read_csv(\"../input/foursquare-location-matching/test.csv\", dtype={\n",
    "    \"id\": str, \"name\": str, \"latitude\": float, \"longitude\": float, \"address\": str, \n",
    "    \"city\": str, \"state\": str, \"zip\": str, \"country\": str, \"url\": str, \"phone\": str, \"categories\": str\n",
    "})\n",
    "\n",
    "# train.csv\n",
    "df_train = cudf.read_csv(\"../input/foursquare-location-matching/train.csv\", dtype={\n",
    "    \"id\": str, \"name\": str, \"latitude\": float, \"longitude\": float, \"address\": str, \n",
    "    \"city\": str, \"state\": str, \"zip\": str, \"country\": str, \"url\": str, \"phone\": str, \"categories\": str, \"point_of_interest\": str\n",
    "}).drop(columns=[\"point_of_interest\"])\n",
    "\n",
    "# 全量数据\n",
    "df_all = cudf.concat([df, df_train], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac4a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:22.394238Z",
     "iopub.status.busy": "2022-07-05T14:13:22.393067Z",
     "iopub.status.idle": "2022-07-05T14:13:23.755131Z",
     "shell.execute_reply": "2022-07-05T14:13:23.754626Z",
     "shell.execute_reply.started": "2022-07-05T14:03:53.702113Z"
    },
    "papermill": {
     "duration": 1.404492,
     "end_time": "2022-07-05T14:13:23.755268",
     "exception": false,
     "start_time": "2022-07-05T14:13:22.350776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Creates new columns converting coordinate degrees to radians.\n",
    "for column in [\"latitude\", \"longitude\"]:\n",
    "    # 将经度或纬度 --> 弧度\n",
    "    rad = np.deg2rad(df_all[column].to_pandas().values) # df_all\n",
    "    df_all[f'{column}_rad'] = rad \n",
    "    rad = np.deg2rad(df[column].to_pandas().values) # df\n",
    "    df[f'{column}_rad'] = rad\n",
    "\n",
    "\n",
    "k = 11\n",
    "ball = BallTree(df_all[[\"latitude_rad\", \"longitude_rad\"]].to_pandas().values, metric='haversine') # 创建BallTree\n",
    "D_latlon, I_latlon = ball.query(df[[\"latitude_rad\", \"longitude_rad\"]].to_pandas().values, k=k)  # 计算每个点到其他点的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e652a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:23.830563Z",
     "iopub.status.busy": "2022-07-05T14:13:23.829974Z",
     "iopub.status.idle": "2022-07-05T14:13:24.004904Z",
     "shell.execute_reply": "2022-07-05T14:13:24.005422Z",
     "shell.execute_reply.started": "2022-07-05T14:03:56.297061Z"
    },
    "papermill": {
     "duration": 0.216493,
     "end_time": "2022-07-05T14:13:24.005572",
     "exception": false,
     "start_time": "2022-07-05T14:13:23.789079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fill_columns = [\"city\", \"state\", \"country\"] # 填充的列\n",
    "print(\"Before fillna\") # 打印前几行\n",
    "print(df[fill_columns].isnull().sum() / df.shape[0]) # 打印缺失值比例\n",
    "for i in range(1, 6):  # 使用前5个candidate\n",
    "    nearest_index = I_latlon[:, i] # 取出第i个candidate\n",
    "    for c in fill_columns:\n",
    "        df[c] = df[c].fillna(df_all.loc[nearest_index, c].reset_index(drop=True)) # 对空值填充第i个candidate的\"city\", \"state\", \"country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8eb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:24.081768Z",
     "iopub.status.busy": "2022-07-05T14:13:24.078581Z",
     "iopub.status.idle": "2022-07-05T14:13:25.357703Z",
     "shell.execute_reply": "2022-07-05T14:13:25.358175Z",
     "shell.execute_reply.started": "2022-07-05T14:03:56.543699Z"
    },
    "papermill": {
     "duration": 1.318549,
     "end_time": "2022-07-05T14:13:25.358324",
     "exception": false,
     "start_time": "2022-07-05T14:13:24.039775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in [\"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\"]:\n",
    "    df[col] = df[col].fillna(\"\") # 对空值填充空字符串\n",
    "\n",
    "# fulltext = 拼接name、address、city、state、country、categories\n",
    "df[\"fulltext\"] = (\n",
    "    df[\"name\"] + \" \" + df[\"address\"] + \" \" + df[\"city\"] + \" \" + df[\"state\"] + \" \"  + df[\"country\"] + \" \" + df[\"categories\"]\n",
    ").to_pandas().replace(r'\\s+', ' ', regex=True) # \n",
    "\n",
    "# Standardization of coordinates.\n",
    "# https://datascience.stackexchange.com/questions/13567/ways-to-deal-with-longitude-latitude-feature\n",
    "df[\"coord_x\"] = cupy.cos(df[\"latitude\"]) * cupy.cos(df[\"longitude\"]) # 经度和纬度转换成x坐标\n",
    "df[\"coord_y\"] = cupy.cos(df[\"latitude\"]) * cupy.sin(df[\"longitude\"]) # 经度和纬度转换成y坐标\n",
    "df[\"coord_z\"] = cupy.sin(df[\"latitude\"]) # 经度和纬度转换成z坐标\n",
    "                       \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1799d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:25.433852Z",
     "iopub.status.busy": "2022-07-05T14:13:25.433149Z",
     "iopub.status.idle": "2022-07-05T14:13:25.438405Z",
     "shell.execute_reply": "2022-07-05T14:13:25.437951Z",
     "shell.execute_reply.started": "2022-07-05T14:03:57.902007Z"
    },
    "papermill": {
     "duration": 0.045438,
     "end_time": "2022-07-05T14:13:25.438520",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.393082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa9486",
   "metadata": {
    "papermill": {
     "duration": 0.034606,
     "end_time": "2022-07-05T14:13:25.578109",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.543503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load ArcFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe40834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:25.658204Z",
     "iopub.status.busy": "2022-07-05T14:13:25.657332Z",
     "iopub.status.idle": "2022-07-05T14:13:25.659185Z",
     "shell.execute_reply": "2022-07-05T14:13:25.659626Z",
     "shell.execute_reply.started": "2022-07-05T14:03:57.918656Z"
    },
    "papermill": {
     "duration": 0.04742,
     "end_time": "2022-07-05T14:13:25.659774",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.612354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FourSquareDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.fulltext = df['fulltext'].values # 全文\n",
    "        self.latitudes = df['latitude'].values # 纬度\n",
    "        self.longitudes = df['longitude'].values # 经度\n",
    "        self.coord_x = df['coord_x'].values # 经纬度坐标 x\n",
    "        self.coord_y = df['coord_y'].values # 经纬度坐标 y\n",
    "        self.coord_z = df['coord_z'].values # 经纬度坐标 z\n",
    "        self.tokenizer = tokenizer # 词表\n",
    "        self.max_length = max_length # 最大长度\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fulltext) # 返回数据长度\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fulltext = self.fulltext[index] # 全文\n",
    "        latitude = self.latitudes[index] # 纬度\n",
    "        longitude = self.longitudes[index] # 经度\n",
    "        coord_x = self.coord_x[index] # 经纬度坐标 x\n",
    "        coord_y = self.coord_y[index] # 经纬度坐标 y\n",
    "        coord_z = self.coord_z[index] # 经纬度坐标 z\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            fulltext, # 全文\n",
    "            truncation=True, # 截断\n",
    "            add_special_tokens=True, # 添加特殊字符\n",
    "            max_length=self.max_length, # 最大长度\n",
    "            padding='max_length', # 填充方式\n",
    "            return_tensors=\"pt\" # 返回张量\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': inputs['input_ids'][0], # 输入张量\n",
    "            'mask': inputs['attention_mask'][0], # 注意力掩码\n",
    "            'latitude': torch.tensor(latitude, dtype=torch.float), # 纬度\n",
    "            'longitude': torch.tensor(longitude, dtype=torch.float), # 经度\n",
    "            'coord_x': torch.tensor(coord_x), # 经纬度坐标 x\n",
    "            'coord_y': torch.tensor(coord_y), # 经纬度坐标 y\n",
    "            'coord_z': torch.tensor(coord_z), # 经纬度坐标 z\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ddfb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:25.749275Z",
     "iopub.status.busy": "2022-07-05T14:13:25.748455Z",
     "iopub.status.idle": "2022-07-05T14:13:25.750914Z",
     "shell.execute_reply": "2022-07-05T14:13:25.750455Z",
     "shell.execute_reply.started": "2022-07-05T14:03:57.942142Z"
    },
    "papermill": {
     "duration": 0.056618,
     "end_time": "2022-07-05T14:13:25.751036",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.694418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arcface\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features  # input的维度\n",
    "        self.out_features = out_features # output的维度\n",
    "        self.s = s # re-scale\n",
    "        self.m = m # margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        # 初始化权重\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin # easy_margin 模式\n",
    "        self.cos_m = math.cos(m) # cos margin\n",
    "        self.sin_m = math.sin(m) # sin margin\n",
    "        self.threshold = math.cos(math.pi - m) # cos(pi - m) = -cos(m)\n",
    "        self.mm = math.sin(math.pi - m) * m # sin(pi - m)*m = sin(m)*m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) # 获得cosθ (vector)\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)) # 获得cosθ\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m # cosθ*cosm – sinθ*sinm = cos(θ + m)\n",
    "        phi = phi.float() # phi to float\n",
    "        cosine = cosine.float() # cosine to float\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            # 以下代码控制了 θ+m 应该在 range[0, pi]\n",
    "            # if cos(θ) > cos(pi - m) means θ + m < math.pi, so phi = cos(θ + m);\n",
    "            # else means θ + m >= math.pi, we use Talyer extension to approximate the cos(θ + m).\n",
    "            # if fact, cos(θ + m) = cos(θ) - m * sin(θ) >= cos(θ) - m * sin(math.pi - m)\n",
    "            phi = torch.where(cosine > self.threshold, phi, cosine - self.mm) # https://github.com/ronghuaiyang/arcface-pytorch/issues/48\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        # 对label形式进行转换，假设batch为2、有3类的话，即将label从[1,2]转换成[[0,1,0],[0,0,1]]\n",
    "        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        # 进行re-scale\n",
    "        output *= self.s\n",
    "\n",
    "        return output\n",
    "\n",
    "class FSMultiModalNet(nn.Module):\n",
    "    def __init__(self, model_name, fc_dim, num_features=3):\n",
    "        super(FSMultiModalNet, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name) # 加载预训练模型\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name, config=self.config) # 加载预训练模型\n",
    "        # self.embedding = nn.Linear(self.config.hidden_size + 2, embedding_size)\n",
    "\n",
    "        self.fc = nn.Linear(self.bert_model.config.hidden_size + num_features, fc_dim) # 全连接层 hidden_size + x'y'z'\n",
    "        self.bn = nn.BatchNorm1d(fc_dim) # BatchNorm1d\n",
    "        self._init_params() # 初始化参数\n",
    "\n",
    "        self.margin = ArcMarginProduct(\n",
    "            fc_dim, # 输入维度\n",
    "            CFG.n_classes, # 输出维度\n",
    "            s=CFG.s, # scale\n",
    "            m=CFG.m, # margin \n",
    "            easy_margin=CFG.easy_margin, # easy_margin\n",
    "            ls_eps=CFG.ls_eps # label smoothing epsilon\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight) # 初始化全连接层权重\n",
    "        nn.init.constant_(self.fc.bias, 0) # 初始化全连接层偏置\n",
    "        nn.init.constant_(self.bn.weight, 1) # 初始化 BatchNorm1d 权重\n",
    "        nn.init.constant_(self.bn.bias, 0) # 初始化 BatchNorm1d 偏置\n",
    "\n",
    "    def forward(self, ids, mask, lat, lon, coord_x, coord_y, coord_z, labels):\n",
    "        feature = self.extract_feature(ids, mask, lat, lon, coord_x, coord_y, coord_z) # 提取特征\n",
    "        output = self.margin(feature, labels) # ArcMarginProduct 输出\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def extract_feature(self, input_ids, attention_mask, lat, lon, coord_x, coord_y, coord_z):\n",
    "        x = self.bert_model(input_ids=input_ids, attention_mask=attention_mask) # 获取 BERT 特征\n",
    "        x = torch.sum(x.last_hidden_state * attention_mask.unsqueeze(-1), dim=1) / attention_mask.sum(dim=1, keepdims=True) # 将 BERT 特征attention_mask部分求平均\n",
    "\n",
    "        x = torch.cat([x, coord_x.view(-1, 1), coord_y.view(-1, 1), coord_z.view(-1, 1)], axis=1) # 将 bert输出 和 x'y'z' 合并\n",
    "\n",
    "        x = self.fc(x) # 全连接层\n",
    "        x = self.bn(x) # BatchNorm1d\n",
    "\n",
    "        return x # 返回特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc9dce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:25.834652Z",
     "iopub.status.busy": "2022-07-05T14:13:25.833751Z",
     "iopub.status.idle": "2022-07-05T14:13:25.835514Z",
     "shell.execute_reply": "2022-07-05T14:13:25.835900Z",
     "shell.execute_reply.started": "2022-07-05T14:03:57.981422Z"
    },
    "papermill": {
     "duration": 0.049526,
     "end_time": "2022-07-05T14:13:25.836039",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.786513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embed(model,tokenizer):\n",
    "    # NN embeddings \n",
    "    dataset = FourSquareDataset(df.to_pandas(), tokenizer=tokenizer, max_length=CFG.max_length) # Dataset\n",
    "    loader = DataLoader(dataset, batch_size=512, num_workers=6, shuffle=False, pin_memory=True) # DataLoader\n",
    "\n",
    "    embeds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader): \n",
    "            ids = data['ids'].to(CFG.device, dtype=torch.long) # input_ids\n",
    "            mask = data['mask'].to(CFG.device, dtype=torch.long) # attention_mask\n",
    "\n",
    "            latitude = data['latitude'].to(CFG.device, dtype=torch.float) # latitude\n",
    "            longitude = data['longitude'].to(CFG.device, dtype=torch.float) # longitude\n",
    "            coord_x = data['coord_x'].to(CFG.device, dtype=torch.float) # coord_x\n",
    "            coord_y = data['coord_y'].to(CFG.device, dtype=torch.float) # coord_y\n",
    "            coord_z = data['coord_z'].to(CFG.device, dtype=torch.float) # coord_z\n",
    "            # labels = data['label'].to(CFG.device, dtype=torch.long)\n",
    "\n",
    "            emb = model.extract_feature(ids, mask, latitude, longitude, coord_x, coord_y, coord_z) # embeddings\n",
    "            embeds.append(emb.detach().cpu().numpy()) # embeddings list\n",
    "\n",
    "    V_embed_bert = cupy.array(np.concatenate(embeds)) # concatenate embeddings --> array\n",
    "    V_embed_bert = V_embed_bert / cupy.linalg.norm(V_embed_bert, ord=2, axis=1, keepdims=True) # l2 normalization\n",
    "    return V_embed_bert # 返回 embeddings\n",
    "\n",
    "\n",
    "def get_embed_model(model_name,fcdim, state_dict,tokenizer):\n",
    "    model = FSMultiModalNet(model_name, fcdim) # 初始化模型\n",
    "    model.to(CFG.device); # 将模型转到GPU\n",
    "    model.load_state_dict(torch.load(state_dict)) # 加载模型参数\n",
    "    embed = get_embed(model,tokenizer) # 获取embedding\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a95125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:13:25.912799Z",
     "iopub.status.busy": "2022-07-05T14:13:25.911934Z",
     "iopub.status.idle": "2022-07-05T14:16:57.321965Z",
     "shell.execute_reply": "2022-07-05T14:16:57.321434Z",
     "shell.execute_reply.started": "2022-07-05T14:03:58.001944Z"
    },
    "papermill": {
     "duration": 211.45033,
     "end_time": "2022-07-05T14:16:57.322102",
     "exception": false,
     "start_time": "2022-07-05T14:13:25.871772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding1\n",
    "embed1 = get_embed_model(CFG_Model1.model_name, 320, CFG_Model1.weight, CFG_Model1.tokenizer)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Embedding2\n",
    "embed2 = get_embed_model(CFG_Model2.model_name, 320, CFG_Model2.weight, CFG_Model2.tokenizer)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Embedding3\n",
    "embed3 = get_embed_model(CFG_Model3.model_name, 320, CFG_Model3.weight, CFG_Model3.tokenizer)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Embedding4\n",
    "embed4 = get_embed_model(CFG_Model4.model_name, 320, CFG_Model4.weight, CFG_Model4.tokenizer)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e0fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:16:57.453551Z",
     "iopub.status.busy": "2022-07-05T14:16:57.411451Z",
     "iopub.status.idle": "2022-07-05T14:16:58.097876Z",
     "shell.execute_reply": "2022-07-05T14:16:58.097354Z",
     "shell.execute_reply.started": "2022-07-05T14:07:50.934750Z"
    },
    "papermill": {
     "duration": 0.733563,
     "end_time": "2022-07-05T14:16:58.098019",
     "exception": false,
     "start_time": "2022-07-05T14:16:57.364456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# l2 normalization and concat\n",
    "V_embed_concat = cupy.concatenate([\n",
    "    embed1, # Embedding1\n",
    "    embed2, # Embedding2\n",
    "    embed3, # Embedding3\n",
    "    embed4, # Embedding4\n",
    "], axis=1)\n",
    "\n",
    "del embed1,embed2,embed3,embed4\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "V_embed_concat = V_embed_concat / cupy.linalg.norm(V_embed_concat, ord=2, axis=1, keepdims=True) # l2 normalization\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7d5dd",
   "metadata": {
    "papermill": {
     "duration": 0.041627,
     "end_time": "2022-07-05T14:16:58.181752",
     "exception": false,
     "start_time": "2022-07-05T14:16:58.140125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate candidate rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6de589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:16:58.275020Z",
     "iopub.status.busy": "2022-07-05T14:16:58.274127Z",
     "iopub.status.idle": "2022-07-05T14:16:58.553046Z",
     "shell.execute_reply": "2022-07-05T14:16:58.552546Z",
     "shell.execute_reply.started": "2022-07-05T14:07:51.974882Z"
    },
    "papermill": {
     "duration": 0.330616,
     "end_time": "2022-07-05T14:16:58.553195",
     "exception": false,
     "start_time": "2022-07-05T14:16:58.222579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create candidate index country by country\n",
    "import faiss\n",
    "\n",
    "def gen_candidate_ranks(df, V_embed, n_candidates, no_country=False):\n",
    "    '''\n",
    "    Generate candidate ranks for each row in df.\n",
    "    '''\n",
    "    res = faiss.StandardGpuResources() # GPU资源\n",
    "    index = faiss.IndexFlatIP(V_embed.shape[1])  # 创建索引\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index) # 将索引放到GPU上\n",
    "    index.add(V_embed) # 将embedding加入索引\n",
    "    D, I = index.search(V_embed, CFG.n_candidates) # 搜索最近的n_candidates个索引\n",
    "    I = np.where(I == -1, 0, I) # 将-1替换为0\n",
    "    D = np.clip(D, 0, 1) # 将D值限制在0和1之间\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8917066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:16:58.641317Z",
     "iopub.status.busy": "2022-07-05T14:16:58.640534Z",
     "iopub.status.idle": "2022-07-05T14:17:40.652271Z",
     "shell.execute_reply": "2022-07-05T14:17:40.651777Z",
     "shell.execute_reply.started": "2022-07-05T14:07:52.348349Z"
    },
    "papermill": {
     "duration": 42.057601,
     "end_time": "2022-07-05T14:17:40.652404",
     "exception": false,
     "start_time": "2022-07-05T14:16:58.594803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 寻找每个样本的n个最近邻值的距离和索引\n",
    "D_concat, I_concat = gen_candidate_ranks(df, cupy.asnumpy(V_embed_concat), CFG.n_candidates)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915bb62",
   "metadata": {
    "papermill": {
     "duration": 0.040206,
     "end_time": "2022-07-05T14:17:40.734657",
     "exception": false,
     "start_time": "2022-07-05T14:17:40.694451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DBA/QE weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79367c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:40.823520Z",
     "iopub.status.busy": "2022-07-05T14:17:40.822967Z",
     "iopub.status.idle": "2022-07-05T14:17:41.543491Z",
     "shell.execute_reply": "2022-07-05T14:17:41.542577Z",
     "shell.execute_reply.started": "2022-07-05T14:08:50.919899Z"
    },
    "papermill": {
     "duration": 0.767892,
     "end_time": "2022-07-05T14:17:41.543648",
     "exception": false,
     "start_time": "2022-07-05T14:17:40.775756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/lyakaap/2nd-place-solution/notebook\n",
    "def query_expansion(V, D, I, alpha=3, k=2):\n",
    "    '''\n",
    "    对V_embed_concat进行查询扩展. \n",
    "    当前样本的embedding, 用最邻近的2个embedding乘以距离权重, 然后求和.\n",
    "\n",
    "    V: embedding matrix\n",
    "    D: distance matrix\n",
    "    I: index matrix\n",
    "    alpha: 次方系数\n",
    "    k: Top-k 邻近值\n",
    "    '''\n",
    "    weights = cupy.array(np.expand_dims(D[:, :k] ** alpha, axis=-1).astype(np.float32)) # 每个样本前k个邻近距离的3次方， shape==(1138812, 2, 1) \n",
    "    chunk_size = 100_000 # chunk大小, 防止OOM\n",
    "    for i in range(0, len(df), chunk_size): \n",
    "        # V.shape: (1138812, 960)\n",
    "        # I[i:i+chunk_size, :k].shape: (100000, 2) # 纯作为索引\n",
    "        # V[I[i:i+chunk_size, :k]].shape: (100000, 2, 960)\n",
    "        # weights[i:i+chunk_size].shape: (100000, 2, 1)\n",
    "        V[i:i+chunk_size] = (V[I[i:i+chunk_size, :k]] * weights[i:i+chunk_size]).sum(axis=1) # 将V中的2个embedding乘上，以距离为权重，然后求和\n",
    "    return V\n",
    "\n",
    "\n",
    "V_embed_concat = query_expansion(V_embed_concat, D_concat, I_concat) # 对V_embed_concat进行查询扩展. 当前样本的embedding, 用最邻近的2个embedding乘以距离权重, 然后求和.\n",
    "V_embed_concat /= np.linalg.norm(V_embed_concat, 2, axis=1, keepdims=True) # l2 正则化\n",
    "\n",
    "del D_concat,I_concat # 删除变量\n",
    "gc.collect() # 清空缓存\n",
    "torch.cuda.empty_cache() # 清空显存\n",
    "\n",
    "D_concat, I_concat = gen_candidate_ranks(df, cupy.asnumpy(V_embed_concat), CFG.n_candidates) # 对V_embed_concat进行索引\n",
    "\n",
    "gc.collect() # 清空缓存\n",
    "torch.cuda.empty_cache() # 清空显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307ec33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:42.392594Z",
     "iopub.status.busy": "2022-07-05T14:17:42.391549Z",
     "iopub.status.idle": "2022-07-05T14:17:42.393705Z",
     "shell.execute_reply": "2022-07-05T14:17:42.394207Z",
     "shell.execute_reply.started": "2022-07-05T14:08:53.221400Z"
    },
    "papermill": {
     "duration": 0.052071,
     "end_time": "2022-07-05T14:17:42.394338",
     "exception": false,
     "start_time": "2022-07-05T14:17:42.342267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "def gen_candidate_rank_spatial(df, n_candidates):\n",
    "    '''\n",
    "    对每个country内的样本进行k邻近查找\n",
    "    按经度/纬度的haversine距离查找candidates。\n",
    "    Returns:\n",
    "        I (np.array): I[s_index][r] -> s_index 的第r个最近的点的索引。\n",
    "    '''\n",
    "    for column in df[[\"latitude\", \"longitude\"]]:\n",
    "        rad = np.deg2rad(df[column].values) # 将经度或纬度 --> 弧度\n",
    "        df[f'{column}_rad'] = rad # 弧度 存入df\n",
    "\n",
    "    I = np.full((len(df), n_candidates), -1, dtype=np.int32) # 创建I矩阵, shape=(sample_number, n_candidates), 全为-1\n",
    "    for country, country_df in tqdm(df[[\"country\", \"latitude_rad\", \"longitude_rad\"]].to_pandas().groupby(\"country\")): # 对每个国家进行遍历\n",
    "        clip_n_candidates = min(len(country_df), n_candidates) # 最大只允许n_candidates个样本\n",
    "        country_df = country_df.reset_index() # 重置索引\n",
    "        ball = BallTree(country_df[[\"latitude_rad\", \"longitude_rad\"]].values, metric='haversine') # 创建BallTree\n",
    "        # 计算country_df中每个点到其他点的距离\n",
    "        _, indices = ball.query( \n",
    "            country_df[[\"latitude_rad\", \"longitude_rad\"]].values,  # 待查询点的经纬度\n",
    "            k = clip_n_candidates # k个最邻近点\n",
    "        )\n",
    "        \n",
    "        indices = np.concatenate(\n",
    "            [\n",
    "                indices,  # BallTree查询结果索引\n",
    "                np.zeros((len(indices), n_candidates - clip_n_candidates), dtype=np.int32) # 当candidates列数过少时，用0补全至n_candidates列数\n",
    "            ], axis=1\n",
    "        )\n",
    "        for i in range(n_candidates):\n",
    "            I[country_df[\"index\"].values, i] = country_df.loc[indices[:, i], \"index\"].values\n",
    "\n",
    "    return I\n",
    "\n",
    "I_spatial = gen_candidate_rank_spatial(df, 10) # 对每个country内的样本进行k邻近查找 shpe==(1138812, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd1025",
   "metadata": {
    "papermill": {
     "duration": 0.041548,
     "end_time": "2022-07-05T14:17:42.611079",
     "exception": false,
     "start_time": "2022-07-05T14:17:42.569531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f27187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:42.704690Z",
     "iopub.status.busy": "2022-07-05T14:17:42.704070Z",
     "iopub.status.idle": "2022-07-05T14:17:49.133616Z",
     "shell.execute_reply": "2022-07-05T14:17:49.134249Z",
     "shell.execute_reply.started": "2022-07-05T14:08:53.359982Z"
    },
    "papermill": {
     "duration": 6.481252,
     "end_time": "2022-07-05T14:17:49.134412",
     "exception": false,
     "start_time": "2022-07-05T14:17:42.653160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer() # 创建tfidf\n",
    "V_name = tfidf.fit_transform(df[\"name\"])  # 对name进行tfidf\n",
    "V_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47686d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:49.234804Z",
     "iopub.status.busy": "2022-07-05T14:17:49.233838Z",
     "iopub.status.idle": "2022-07-05T14:17:49.316430Z",
     "shell.execute_reply": "2022-07-05T14:17:49.317024Z",
     "shell.execute_reply.started": "2022-07-05T14:09:00.585975Z"
    },
    "papermill": {
     "duration": 0.135718,
     "end_time": "2022-07-05T14:17:49.317172",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.181454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english') # 创建tfidf\n",
    "df[\"full_address\"] = df[\"address\"] + \", \" + df[\"city\"] + \", \" + df[\"state\"] + \", \"  + df[\"country\"] # 创建full_address列, 为 address, city, state, country\n",
    "V_full_address = tfidf.fit_transform(df[\"full_address\"]) # 对full_address进行tfidf\n",
    "V_full_address.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a52d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:49.415344Z",
     "iopub.status.busy": "2022-07-05T14:17:49.414302Z",
     "iopub.status.idle": "2022-07-05T14:17:49.481134Z",
     "shell.execute_reply": "2022-07-05T14:17:49.481812Z",
     "shell.execute_reply.started": "2022-07-05T14:09:00.715109Z"
    },
    "papermill": {
     "duration": 0.118498,
     "end_time": "2022-07-05T14:17:49.481982",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.363484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() # 创建tfidf\n",
    "V_cat = tfidf.fit_transform(df[\"categories\"]) # 对categories进行tfidf\n",
    "V_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7381fd1",
   "metadata": {
    "papermill": {
     "duration": 0.046913,
     "end_time": "2022-07-05T14:17:49.576183",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.529270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4fecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:49.675561Z",
     "iopub.status.busy": "2022-07-05T14:17:49.673952Z",
     "iopub.status.idle": "2022-07-05T14:17:49.676246Z",
     "shell.execute_reply": "2022-07-05T14:17:49.676747Z",
     "shell.execute_reply.started": "2022-07-05T14:09:00.803011Z"
    },
    "papermill": {
     "duration": 0.054354,
     "end_time": "2022-07-05T14:17:49.676886",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.622532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cuml import ForestInference\n",
    "\n",
    "THRESHOLD = 0.3 # 阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c7014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:49.812610Z",
     "iopub.status.busy": "2022-07-05T14:17:49.805291Z",
     "iopub.status.idle": "2022-07-05T14:17:49.815203Z",
     "shell.execute_reply": "2022-07-05T14:17:49.814603Z",
     "shell.execute_reply.started": "2022-07-05T14:09:00.811372Z"
    },
    "papermill": {
     "duration": 0.0911,
     "end_time": "2022-07-05T14:17:49.815341",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.724241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = 6371.0  # 地球半径, 单位km\n",
    "\n",
    "def manhattan(lat1, long1, lat2, long2):\n",
    "    '''\n",
    "    计算曼哈顿距离\n",
    "    '''\n",
    "    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n",
    "\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/justfor/speedup-haversine/script\n",
    "    计算地球上两点之间的大圆距离的距离（以小数点后的度数指定）。\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1 # 经度差\n",
    "    dlat = lat2 - lat1 # 纬度差\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return km\n",
    "\n",
    "\n",
    "def create_features(df, i, indices):\n",
    "    '''\n",
    "    创建特征\n",
    "    '''\n",
    "    prev_i = max(i-1, 0) # i-1,最小值为0\n",
    "    next_i = min(i+1, indices.shape[1] - 1) # i+1,最大值为indices长度\n",
    "    \n",
    "    prev_cand_index = indices[:, prev_i] # 第i-1个相似度的candidate的索引\n",
    "    next_cand_index = indices[:, next_i] # 第i+1个相似度的candidate的索引\n",
    "    cand_index = indices[:, i] # 第i个相似度的candidate索引\n",
    "    \n",
    "    lon1 = df[\"longitude\"].to_pandas().to_numpy() # longitude列的值\n",
    "    lat1 = df[\"latitude\"].to_pandas().to_numpy() # latitude列的值\n",
    "    lon2 = df[\"longitude\"][cand_index].to_pandas().to_numpy() # 第i个相似度的candidate顺序的longitude列值\n",
    "    lat2 = df[\"latitude\"][cand_index].to_pandas().to_numpy() # 第i个相似度的candidate顺序的latitude列值\n",
    "    #df[\"diff_lon\"] = lon1 - lon2\n",
    "    #df[\"diff_lat\"] = lat1 - lat2\n",
    "    df[\"diff_lon\"] = np.abs(lon1 - lon2) # 经度差绝对值\n",
    "    df[\"diff_lat\"] = np.abs(lat1 - lat2) # 纬度差绝对值\n",
    "    \n",
    "    df[\"lonlat_eucdist\"] =  (df['diff_lon'] ** 2 + df['diff_lat'] ** 2) ** 0.5 # 经纬度差的平方根\n",
    "    df[\"lonlat_manhattan\"] = manhattan(lat1, lon1, lat2, lon2) # 曼哈顿距离\n",
    "    df[\"lonlat_haversine_dist\"] = haversine_np(lon1, lat1, lon2, lat2) # 地球距离\n",
    "    \n",
    "    df[\"name_cossim\"] = V_name.multiply(V_name[cand_index]).sum(axis=1).ravel() # 名称相似度\n",
    "    df[\"full_address_cossim\"] = V_full_address.multiply(V_full_address[cand_index]).sum(axis=1).ravel() # 地址相似度\n",
    "    df[\"cat_cossim\"] = V_cat.multiply(V_cat[cand_index]).sum(axis=1).ravel() # 类别相似度\n",
    "    \n",
    "    df[\"cand_hit_count_02\"] = df[\"hit_count_02\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.2的样本的candidate的概率\n",
    "    df[\"cand_hit_count_03\"] = df[\"hit_count_03\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.3的样本的candidate的概率\n",
    "    df[\"cand_hit_count_04\"] = df[\"hit_count_04\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.4的样本的candidate的概率 \n",
    "    df[\"cand_hit_count_05\"] = df[\"hit_count_05\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.5的样本的candidate的概率\n",
    "    df[\"cand_hit_count_sum\"] = df[\"hit_count_sum\"][cand_index].to_pandas().to_numpy() # 所有样本的candidate的概率\n",
    "    \n",
    "    df[\"hit_count_02_min\"] = df[[\"hit_count_02\", \"cand_hit_count_02\"]].min(axis=1)  # 样本预测概率和其candidate的概率 的最小值（0.2）\n",
    "    df[\"hit_count_03_min\"] = df[[\"hit_count_03\", \"cand_hit_count_03\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.3）\n",
    "    df[\"hit_count_04_min\"] = df[[\"hit_count_04\", \"cand_hit_count_04\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.4）\n",
    "    df[\"hit_count_05_min\"] = df[[\"hit_count_05\", \"cand_hit_count_05\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.5）\n",
    "    df[\"hit_count_sum_min\"] = df[[\"hit_count_sum\", \"cand_hit_count_sum\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（所有样本）\n",
    "    \n",
    "    df[\"hit_count_02_max\"] = df[[\"hit_count_02\", \"cand_hit_count_02\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.2）\n",
    "    df[\"hit_count_03_max\"] = df[[\"hit_count_03\", \"cand_hit_count_03\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.3）\n",
    "    df[\"hit_count_04_max\"] = df[[\"hit_count_04\", \"cand_hit_count_04\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.4）\n",
    "    df[\"hit_count_05_max\"] = df[[\"hit_count_05\", \"cand_hit_count_05\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.5）\n",
    "    df[\"hit_count_sum_max\"] = df[[\"hit_count_sum\", \"cand_hit_count_sum\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（所有样本）\n",
    "    \n",
    "    cossim = []\n",
    "    eucdist = []\n",
    "    \n",
    "    eucdist1=[]\n",
    "    eucdist2=[]\n",
    "    eucdist3=[]\n",
    "    eucdist4=[]\n",
    "    eucdist5=[]\n",
    "    \n",
    "    chunk_size = 100_000 # 每次处理的数据量\n",
    "    for i in range(0, len(df), chunk_size):  # 以chunk取数据，防止OOM \n",
    "        cossim.append(cupy.multiply(V_embed_concat[i:i+chunk_size], V_embed_concat[cand_index[i:i+chunk_size]]).sum(axis=1)) # V_embed_concat 相似度\n",
    "        eucdist.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # V_embed_concat 欧式距离 cur - cand_index\n",
    "        \n",
    "        eucdist1.append(cupy.sqrt(((V_embed_concat[prev_cand_index[i:i+chunk_size]] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离1 prev_cand_index - cand_index\n",
    "        eucdist2.append(cupy.sqrt(((V_embed_concat[next_cand_index[i:i+chunk_size]] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离2 next_cand_index - cand_index\n",
    "        eucdist3.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size]                  - V_embed_concat[prev_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离3 cur - prev_cand_index\n",
    "        eucdist4.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size]                  - V_embed_concat[next_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离4 cur - next_cand_index\n",
    "        eucdist5.append(cupy.sqrt(((V_embed_concat[prev_cand_index[i:i+chunk_size]] - V_embed_concat[next_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离5 prev_cand_index - next_cand_index\n",
    "    \n",
    "    # 将cupy数据转换为pandas数据\n",
    "    df[\"embed_cossim\"] = cupy.concatenate(cossim) \n",
    "    df[\"embed_eucdist\"] = cupy.concatenate(eucdist)\n",
    "    df[\"embed_eucdist1\"] = cupy.concatenate(eucdist1)\n",
    "    df[\"embed_eucdist2\"] = cupy.concatenate(eucdist2)\n",
    "    df[\"embed_eucdist3\"] = cupy.concatenate(eucdist3)\n",
    "    df[\"embed_eucdist4\"] = cupy.concatenate(eucdist4)\n",
    "    df[\"embed_eucdist5\"] = cupy.concatenate(eucdist5)\n",
    "    \n",
    "    # 欧式距离差\n",
    "    df[\"d0_d1\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist1\"] \n",
    "    df[\"d0_d2\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist2\"]\n",
    "    df[\"d0_d3\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist3\"]\n",
    "    df[\"d0_d4\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist4\"]\n",
    "    df[\"d0_d5\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist5\"]\n",
    "    \n",
    "    for col in [\"id\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\", \"full_address\"]:\n",
    "        df[f\"{col}_edit_dist\"] = df[col].str.edit_distance(df[col][cand_index]) # Levenshtein 距离\n",
    "        # 标准化 Levenshtein 距离\n",
    "        df[f\"norm_{col}_edit_dist\"] = df[f\"{col}_edit_dist\"] / df[col].str.len() # Levenshtein 距离/长度\n",
    "        df[f\"norm_{col}_edit_dist\"] = df[f\"norm_{col}_edit_dist\"].replace([np.inf, -np.inf], 0) # 将inf替换为0\n",
    "    \n",
    "    features = [\n",
    "        \"diff_lon\",\n",
    "        \"diff_lat\",\n",
    "        \"lonlat_eucdist\",\n",
    "        \"lonlat_manhattan\",\n",
    "        \"lonlat_haversine_dist\",\n",
    "        \"name_cossim\",\n",
    "        \"full_address_cossim\",\n",
    "        \"cat_cossim\",\n",
    "        \"embed_cossim\",\n",
    "        \"embed_eucdist\",\n",
    "        \n",
    "        \"embed_eucdist1\",\n",
    "        \"embed_eucdist2\",\n",
    "        \"embed_eucdist3\",\n",
    "        \"embed_eucdist4\",\n",
    "        \"embed_eucdist5\",\n",
    "        \"d0_d1\",\n",
    "        \"d0_d2\",\n",
    "        \"d0_d3\",\n",
    "        \"d0_d4\",\n",
    "        \"d0_d5\",\n",
    "\n",
    "        \"hit_count_02\",\n",
    "        \"hit_count_03\",\n",
    "        \"hit_count_04\",\n",
    "        \"hit_count_05\",\n",
    "        \"hit_count_sum\",\n",
    "        \n",
    "        \"hit_count_02_min\",\n",
    "        \"hit_count_03_min\",\n",
    "        \"hit_count_04_min\",\n",
    "        \"hit_count_05_min\",\n",
    "        \"hit_count_sum_min\",\n",
    "        \"hit_count_02_max\",\n",
    "        \"hit_count_03_max\",\n",
    "        \"hit_count_04_max\",\n",
    "        \"hit_count_05_max\",\n",
    "        \"hit_count_sum_max\"\n",
    "    ]\n",
    "    \n",
    "    for col in [\"id\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\", \"full_address\"]:\n",
    "        features.append(f\"{col}_edit_dist\") # 存下edit_dist类特征\n",
    "        features.append(f\"norm_{col}_edit_dist\") # 存下标准化后的edit_dist类特征\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac83e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:49.928733Z",
     "iopub.status.busy": "2022-07-05T14:17:49.927040Z",
     "iopub.status.idle": "2022-07-05T14:17:49.929337Z",
     "shell.execute_reply": "2022-07-05T14:17:49.929834Z",
     "shell.execute_reply.started": "2022-07-05T14:10:00.787491Z"
    },
    "papermill": {
     "duration": 0.065756,
     "end_time": "2022-07-05T14:17:49.929990",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.864234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dfs = []\n",
    "\n",
    "def inference(I, n_candiates, prefix, link):\n",
    "    # 初始化预测结果\n",
    "    df[\"hit_count_02\"] = 0\n",
    "    df[\"hit_count_03\"] = 0\n",
    "    df[\"hit_count_04\"] = 0\n",
    "    df[\"hit_count_05\"] = 0\n",
    "    df[\"hit_count_sum\"] = 0\n",
    "    \n",
    "    df[\"cand_hit_count_02\"] = 0\n",
    "    df[\"cand_hit_count_03\"] = 0\n",
    "    df[\"cand_hit_count_04\"] = 0\n",
    "    df[\"cand_hit_count_05\"] = 0\n",
    "    df[\"cand_hit_count_sum\"] = 0\n",
    "    \n",
    "    for i in range(I.shape[1]):\n",
    "        print(f\"Candidate rank {i} ...\")\n",
    "        tmp_df = df.copy() # 复制一份df\n",
    "        tmp_df[\"match_id\"] = tmp_df[\"id\"].to_pandas().values[I[:, i]] # 第i个相似度的candidate的id\n",
    "\n",
    "        tmp_df, features = create_features(tmp_df, i, I) # 创建特征\n",
    "        tmp_df[\"pred\"] = 0 # pred = 0\n",
    "        for fold in range(CFG.n_splits):\n",
    "            print(f\"    ===== fold{fold} =====\")\n",
    "            xgb_model = ForestInference.load(\n",
    "                f\"../input/xgb-4ensemble-0705-v2/fs_xgb_model_{prefix}_candidate{i}_fold{fold}.json\", # 加载模型\n",
    "                output_class=True, # 输出为class\n",
    "                model_type=\"xgboost_json\" # 模型类型\n",
    "            )\n",
    "\n",
    "            pred = xgb_model.predict_proba(tmp_df[features].to_pandas())[:, 1] # 预测\n",
    "            tmp_df[\"pred\"] += pred / CFG.n_splits # 平均\n",
    "\n",
    "#         dfs.append(tmp_df[tmp_df[\"pred\"] > THRESHOLD][[\"id\", \"match_id\", \"pred\"]])\n",
    "\n",
    "        df[\"hit_count_02\"] += (pred > 0.2) # 预测结果大于0.2\n",
    "        df[\"hit_count_03\"] += (pred > 0.3) # 预测结果大于0.3\n",
    "        df[\"hit_count_04\"] += (pred > 0.4) # 预测结果大于0.4 \n",
    "        df[\"hit_count_05\"] += (pred > 0.5) # 预测结果大于0.5\n",
    "        df[\"hit_count_sum\"] += pred # 预测结果\n",
    "        \n",
    "        ids = tmp_df.to_pandas().id.values # id\n",
    "        match_ids = tmp_df.to_pandas().match_id.values # match_id\n",
    "        pred_lis = tmp_df.to_pandas().pred.values # pred\n",
    "        for i in range(len(tmp_df)):\n",
    "            dist = 1 - pred_lis[i] # 距离 = 1 - 预测结果\n",
    "            if dist > 0.9: # 如果距离大于0.9，则跳过\n",
    "                continue\n",
    "            link[ids[i]][match_ids[i]] = min(dist, link[ids[i]][match_ids[i]]) # 更新最小距离\n",
    "        \n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b0a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:17:50.034107Z",
     "iopub.status.busy": "2022-07-05T14:17:50.033324Z",
     "iopub.status.idle": "2022-07-05T14:18:18.699741Z",
     "shell.execute_reply": "2022-07-05T14:18:18.700371Z",
     "shell.execute_reply.started": "2022-07-05T14:10:01.692123Z"
    },
    "papermill": {
     "duration": 28.723163,
     "end_time": "2022-07-05T14:18:18.700584",
     "exception": false,
     "start_time": "2022-07-05T14:17:49.977421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "link = defaultdict(lambda:defaultdict(lambda: 1)) # 结果字典\n",
    "\n",
    "link = inference(I_concat, CFG.n_candidates, \"embed\", link) # I_concat 训练\n",
    "link = inference(I_spatial, CFG.n_spatial_candidates, \"spatial\", link) # I_spatial 训练\n",
    "\n",
    "# link like:\n",
    "# defaultdict(<function __main__.<lambda>()>,\n",
    "#             {'E_00001118ad0191': {'E_00001118ad0191': 1.1920928955078125e-07}),\n",
    "#              'E_000020eb6fed40': {'E_000020eb6fed40': 1.1920928955078125e-07}),\n",
    "#              'E_00002f98667edf': {'E_00002f98667edf': 2.384185791015625e-07}),\n",
    "#              'E_001b6bad66eb98': {'E_001b6bad66eb98': 1.1920928955078125e-07,\n",
    "#                                   'E_0283d9f61e569d': 3.4570693969726562e-06}),\n",
    "#              'E_0283d9f61e569d': {'E_0283d9f61e569d': 1.1920928955078125e-07,\n",
    "#                                   'E_001b6bad66eb98': 2.7418136596679688e-06})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee1f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:18:18.908390Z",
     "iopub.status.busy": "2022-07-05T14:18:18.907588Z",
     "iopub.status.idle": "2022-07-05T14:18:18.910782Z",
     "shell.execute_reply": "2022-07-05T14:18:18.911451Z",
     "shell.execute_reply.started": "2022-07-05T14:10:30.479512Z"
    },
    "papermill": {
     "duration": 0.109197,
     "end_time": "2022-07-05T14:18:18.911665",
     "exception": false,
     "start_time": "2022-07-05T14:18:18.802468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_COST=0.4 \n",
    "\n",
    "id_lis = []\n",
    "matches = []\n",
    "for source_id in tqdm(set(df.to_pandas().id)):\n",
    "    tmp = []\n",
    "    for key,value in link[source_id].items(): # key: match_id, value: dist\n",
    "        if value <= MAX_COST: # 如果距离小于MAX_COST，则加入\n",
    "            tmp.append(key)\n",
    "            \n",
    "    matches.append(tmp)\n",
    "    id_lis.append(source_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = id_lis\n",
    "sub_df[\"match_id\"] = matches\n",
    "sub_df[\"matches\"] = sub_df[\"match_id\"].apply(lambda x: \" \".join(set(x))) # 将match_id转换为字符串\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f65df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T14:18:19.711990Z",
     "iopub.status.busy": "2022-07-05T14:18:19.711183Z",
     "iopub.status.idle": "2022-07-05T14:18:19.718217Z",
     "shell.execute_reply": "2022-07-05T14:18:19.717777Z",
     "shell.execute_reply.started": "2022-07-05T14:10:30.627154Z"
    },
    "papermill": {
     "duration": 0.075455,
     "end_time": "2022-07-05T14:18:19.718331",
     "exception": false,
     "start_time": "2022-07-05T14:18:19.642876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False, columns=[\"id\", \"matches\"]) # 写入文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 363.50342,
   "end_time": "2022-07-05T14:18:23.242645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-05T14:12:19.739225",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
