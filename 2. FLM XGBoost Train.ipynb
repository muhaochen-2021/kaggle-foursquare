{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ca18c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:03:21.983149Z",
     "iopub.status.busy": "2022-07-05T12:03:21.982320Z",
     "iopub.status.idle": "2022-07-05T12:03:51.685678Z",
     "shell.execute_reply": "2022-07-05T12:03:51.685081Z",
     "shell.execute_reply.started": "2022-07-05T11:58:19.268433Z"
    },
    "papermill": {
     "duration": 29.755625,
     "end_time": "2022-07-05T12:03:51.685842",
     "exception": false,
     "start_time": "2022-07-05T12:03:21.930217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9c050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:03:51.763799Z",
     "iopub.status.busy": "2022-07-05T12:03:51.759311Z",
     "iopub.status.idle": "2022-07-05T12:04:02.704497Z",
     "shell.execute_reply": "2022-07-05T12:04:02.704012Z",
     "shell.execute_reply.started": "2022-07-05T11:58:50.617261Z"
    },
    "papermill": {
     "duration": 10.983393,
     "end_time": "2022-07-05T12:04:02.704639",
     "exception": false,
     "start_time": "2022-07-05T12:03:51.721246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# For Transformer Models\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, AutoConfig\n",
    "\n",
    "from cuml.neighbors import NearestNeighbors \n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.metrics import pairwise_distances\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd03de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:02.784594Z",
     "iopub.status.busy": "2022-07-05T12:04:02.782942Z",
     "iopub.status.idle": "2022-07-05T12:04:02.785177Z",
     "shell.execute_reply": "2022-07-05T12:04:02.785616Z",
     "shell.execute_reply.started": "2022-07-05T11:59:02.880689Z"
    },
    "papermill": {
     "duration": 0.046337,
     "end_time": "2022-07-05T12:04:02.785748",
     "exception": false,
     "start_time": "2022-07-05T12:04:02.739411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42 # 随机种子\n",
    "    n_candidates = 50 # 候选集数量\n",
    "    n_splits = 4 # 折数\n",
    "    max_length = 64 # 最大长度\n",
    "    device = torch.device('cuda') # GPU\n",
    "\n",
    "    reuse_dir1 = \"../input/bert_embed_1/\" \n",
    "    reuse_dir2 = \"../input/bert_embed_2/\" \n",
    "    reuse_dir3 = \"../input/bert_embed_3/\"\n",
    "    reuse_dir4 = \"../input/bert_embed_4/\"\n",
    "\n",
    "    # Metric loss and its params \n",
    "    s = 30.0 # arcface 参数 scale\n",
    "    m = 0.5  # arcface 参数 margin\n",
    "    ls_eps = 0.0 # arcface 参数 eps\n",
    "    easy_margin = False # easy_margin\n",
    "    n_classes = 739972 # 分类数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bae45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:02.860103Z",
     "iopub.status.busy": "2022-07-05T12:04:02.858480Z",
     "iopub.status.idle": "2022-07-05T12:04:02.860738Z",
     "shell.execute_reply": "2022-07-05T12:04:02.861160Z",
     "shell.execute_reply.started": "2022-07-05T11:59:02.898449Z"
    },
    "papermill": {
     "duration": 0.042039,
     "end_time": "2022-07-05T12:04:02.861283",
     "exception": false,
     "start_time": "2022-07-05T12:04:02.819244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(data, file_name):\n",
    "    '''\n",
    "    保存 pickle 文件\n",
    "    '''\n",
    "    with open(f\"{file_name}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    '''\n",
    "    加载 pickle 文件\n",
    "    '''\n",
    "    with open(f\"{file_name}.pickle\", \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba2b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:02.936115Z",
     "iopub.status.busy": "2022-07-05T12:04:02.935441Z",
     "iopub.status.idle": "2022-07-05T12:04:02.940267Z",
     "shell.execute_reply": "2022-07-05T12:04:02.939876Z",
     "shell.execute_reply.started": "2022-07-05T11:59:02.956306Z"
    },
    "papermill": {
     "duration": 0.043603,
     "end_time": "2022-07-05T12:04:02.940371",
     "exception": false,
     "start_time": "2022-07-05T12:04:02.896768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed=42):\n",
    "    '''\n",
    "    Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dda273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:03.044011Z",
     "iopub.status.busy": "2022-07-05T12:04:03.042453Z",
     "iopub.status.idle": "2022-07-05T12:04:03.046333Z",
     "shell.execute_reply": "2022-07-05T12:04:03.045935Z",
     "shell.execute_reply.started": "2022-07-05T11:59:02.974582Z"
    },
    "papermill": {
     "duration": 0.072286,
     "end_time": "2022-07-05T12:04:03.046439",
     "exception": false,
     "start_time": "2022-07-05T12:04:02.974153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arcface\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features  # input的维度\n",
    "        self.out_features = out_features # output的维度\n",
    "        self.s = s # re-scale\n",
    "        self.m = m # margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        # 初始化权重\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin # easy_margin 模式\n",
    "        self.cos_m = math.cos(m) # cos margin\n",
    "        self.sin_m = math.sin(m) # sin margin\n",
    "        self.threshold = math.cos(math.pi - m) # cos(pi - m) = -cos(m)\n",
    "        self.mm = math.sin(math.pi - m) * m # sin(pi - m)*m = sin(m)*m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) # 获得cosθ (vector)\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)) # 获得cosθ\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m # cosθ*cosm – sinθ*sinm = cos(θ + m)\n",
    "        phi = phi.float() # phi to float\n",
    "        cosine = cosine.float() # cosine to float\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            # 以下代码控制了 θ+m 应该在 range[0, pi]\n",
    "            # if cos(θ) > cos(pi - m) means θ + m < math.pi, so phi = cos(θ + m);\n",
    "            # else means θ + m >= math.pi, we use Talyer extension to approximate the cos(θ + m).\n",
    "            # if fact, cos(θ + m) = cos(θ) - m * sin(θ) >= cos(θ) - m * sin(math.pi - m)\n",
    "            phi = torch.where(cosine > self.threshold, phi, cosine - self.mm) # https://github.com/ronghuaiyang/arcface-pytorch/issues/48\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        # 对label形式进行转换，假设batch为2、有3类的话，即将label从[1,2]转换成[[0,1,0],[0,0,1]]\n",
    "        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        # 进行re-scale\n",
    "        output *= self.s\n",
    "\n",
    "        return output\n",
    "\n",
    "class FourSquareDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.fulltext = df['fulltext'].values # 全文\n",
    "        self.latitudes = df['latitude'].values # 纬度\n",
    "        self.longitudes = df['longitude'].values # 经度\n",
    "        self.coord_x = df['coord_x'].values # 经纬度坐标 x\n",
    "        self.coord_y = df['coord_y'].values # 经纬度坐标 y\n",
    "        self.coord_z = df['coord_z'].values # 经纬度坐标 z\n",
    "        self.labels = df['point_of_interest'].values # point_of_interest 标签\n",
    "        self.tokenizer = tokenizer # 词表\n",
    "        self.max_length = max_length # 最大长度\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fulltext) # 返回数据长度\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fulltext = self.fulltext[index] # 全文\n",
    "        latitude = self.latitudes[index] # 纬度\n",
    "        longitude = self.longitudes[index] # 经度\n",
    "        label = self.labels[index] # 标签\n",
    "        coord_x = self.coord_x[index] # 经纬度坐标 x\n",
    "        coord_y = self.coord_y[index] # 经纬度坐标 y\n",
    "        coord_z = self.coord_z[index] # 经纬度坐标 z\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            fulltext, # 全文\n",
    "            truncation=True, # 截断\n",
    "            add_special_tokens=True, # 添加特殊字符\n",
    "            max_length=self.max_length, # 最大长度\n",
    "            padding='max_length', # 填充方式\n",
    "            return_tensors=\"pt\" # 返回张量\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': inputs['input_ids'][0], # input_ids\n",
    "            'mask': inputs['attention_mask'][0], # 注意力掩码\n",
    "            'latitude': torch.tensor(latitude, dtype=torch.float), # 纬度\n",
    "            'longitude': torch.tensor(longitude, dtype=torch.float), # 经度\n",
    "            'coord_x': torch.tensor(coord_x), # 经纬度坐标 x\n",
    "            'coord_y': torch.tensor(coord_y), # 经纬度坐标 y\n",
    "            'coord_z': torch.tensor(coord_z), # 经纬度坐标 z\n",
    "            'label': torch.tensor(label, dtype=torch.long) # 标签\n",
    "        }\n",
    "    \n",
    "\n",
    "class FSMultiModalNet(nn.Module):\n",
    "    def __init__(self, model_name, fc_dim, num_features=3):\n",
    "        super(FSMultiModalNet, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name) # 加载预训练模型\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name, config=self.config) # 加载预训练模型\n",
    "        # self.embedding = nn.Linear(self.config.hidden_size + 2, embedding_size)\n",
    "\n",
    "        self.fc = nn.Linear(self.bert_model.config.hidden_size + num_features, fc_dim) # 全连接层 hidden_size + x'y'z'\n",
    "        self.bn = nn.BatchNorm1d(fc_dim) # BatchNorm1d\n",
    "        self._init_params() # 初始化参数\n",
    "\n",
    "        self.margin = ArcMarginProduct(\n",
    "            fc_dim, # 输入维度\n",
    "            CFG.n_classes, # 输出维度\n",
    "            s=CFG.s, # scale\n",
    "            m=CFG.m, # margin \n",
    "            easy_margin=CFG.easy_margin, # easy_margin\n",
    "            ls_eps=CFG.ls_eps # label smoothing epsilon\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight) # 初始化全连接层权重\n",
    "        nn.init.constant_(self.fc.bias, 0) # 初始化全连接层偏置\n",
    "        nn.init.constant_(self.bn.weight, 1) # 初始化 BatchNorm1d 权重\n",
    "        nn.init.constant_(self.bn.bias, 0) # 初始化 BatchNorm1d 偏置\n",
    "\n",
    "    def forward(self, ids, mask, lat, lon, coord_x, coord_y, coord_z, labels):\n",
    "        feature = self.extract_feature(ids, mask, lat, lon, coord_x, coord_y, coord_z) # 提取特征\n",
    "        output = self.margin(feature, labels) # ArcMarginProduct 输出\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def extract_feature(self, input_ids, attention_mask, lat, lon, coord_x, coord_y, coord_z):\n",
    "        x = self.bert_model(input_ids=input_ids, attention_mask=attention_mask) # 获取 BERT 特征\n",
    "        x = torch.sum(x.last_hidden_state * attention_mask.unsqueeze(-1), dim=1) / attention_mask.sum(dim=1, keepdims=True) # 将 BERT 特征attention_mask部分求平均\n",
    "\n",
    "        x = torch.cat([x, coord_x.view(-1, 1), coord_y.view(-1, 1), coord_z.view(-1, 1)], axis=1) # 将 bert输出 和 x'y'z' 合并\n",
    "\n",
    "        x = self.fc(x) # 全连接层\n",
    "        x = self.bn(x) # BatchNorm1d\n",
    "\n",
    "        return x # 返回特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec45bb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:13.557839Z",
     "iopub.status.busy": "2022-07-05T12:04:13.557179Z",
     "iopub.status.idle": "2022-07-05T12:04:24.002582Z",
     "shell.execute_reply": "2022-07-05T12:04:24.003174Z",
     "shell.execute_reply.started": "2022-07-05T11:59:14.358469Z"
    },
    "papermill": {
     "duration": 10.490619,
     "end_time": "2022-07-05T12:04:24.003364",
     "exception": false,
     "start_time": "2022-07-05T12:04:13.512745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"../input/trainfilled/train_filled.csv\") # 经过空值填充的训练集\n",
    "\n",
    "for col in [\"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\"]:\n",
    "    df[col] = df[col].fillna(\"\") # 填充缺失值\n",
    "    \n",
    "# fulltext = 拼接name、address、city、state、country、categories\n",
    "df[\"fulltext\"] = (\n",
    "    df[\"name\"] + \" \" + df[\"address\"] + \" \" + df[\"city\"] + \" \" + df[\"state\"] + \" \"  + df[\"country\"] + \" \" + df[\"categories\"]\n",
    ").to_pandas().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Standardization of coordinates.\n",
    "# https://datascience.stackexchange.com/questions/13567/ways-to-deal-with-longitude-latitude-feature\n",
    "df[\"coord_x\"] = cupy.cos(df[\"latitude\"]) * cupy.cos(df[\"longitude\"]) # 经度和纬度转换成x坐标\n",
    "df[\"coord_y\"] = cupy.cos(df[\"latitude\"]) * cupy.sin(df[\"longitude\"]) # 经度和纬度转换成y坐标\n",
    "df[\"coord_z\"] = cupy.sin(df[\"latitude\"]) # 经度和纬度转换成z坐标\n",
    "                       \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4c04c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:24.080576Z",
     "iopub.status.busy": "2022-07-05T12:04:24.079713Z",
     "iopub.status.idle": "2022-07-05T12:04:27.541018Z",
     "shell.execute_reply": "2022-07-05T12:04:27.540086Z",
     "shell.execute_reply.started": "2022-07-05T11:59:27.207426Z"
    },
    "papermill": {
     "duration": 3.502413,
     "end_time": "2022-07-05T12:04:27.541161",
     "exception": false,
     "start_time": "2022-07-05T12:04:24.038748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder() # 创建标签编码器\n",
    "df['point_of_interest'] = encoder.fit_transform(df['point_of_interest'].to_array()) # 对point_of_interest做编码标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43502ca0",
   "metadata": {
    "papermill": {
     "duration": 0.034383,
     "end_time": "2022-07-05T12:04:27.610115",
     "exception": false,
     "start_time": "2022-07-05T12:04:27.575732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf60c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:04:27.683695Z",
     "iopub.status.busy": "2022-07-05T12:04:27.683056Z",
     "iopub.status.idle": "2022-07-05T12:05:30.031923Z",
     "shell.execute_reply": "2022-07-05T12:05:30.032508Z",
     "shell.execute_reply.started": "2022-07-05T11:59:31.627538Z"
    },
    "papermill": {
     "duration": 62.388503,
     "end_time": "2022-07-05T12:05:30.032724",
     "exception": false,
     "start_time": "2022-07-05T12:04:27.644221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "V_embed_bert1 = load_pickle(f\"{CFG.reuse_dir1}/V_embed_bert\") # 加载bert的embedding1\n",
    "print(V_embed_bert1.shape)\n",
    "\n",
    "V_embed_bert2 = load_pickle(f\"{CFG.reuse_dir2}/V_embed_bert\") # 加载bert的embedding2 \n",
    "print(V_embed_bert2.shape)\n",
    "\n",
    "V_embed_bert3 = load_pickle(f\"{CFG.reuse_dir3}/V_embed_bert\") # 加载bert的embedding3\n",
    "print(V_embed_bert3.shape)\n",
    "\n",
    "V_embed_bert4 = load_pickle(f\"{CFG.reuse_dir4}/V_embed_bert\") # 加载bert的embedding4\n",
    "print(V_embed_bert4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089efecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:05:30.115750Z",
     "iopub.status.busy": "2022-07-05T12:05:30.115030Z",
     "iopub.status.idle": "2022-07-05T12:05:31.152730Z",
     "shell.execute_reply": "2022-07-05T12:05:31.153109Z",
     "shell.execute_reply.started": "2022-07-05T12:00:32.680085Z"
    },
    "papermill": {
     "duration": 1.080101,
     "end_time": "2022-07-05T12:05:31.153257",
     "exception": false,
     "start_time": "2022-07-05T12:05:30.073156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate\n",
    "V_embed_concat = cupy.concatenate([\n",
    "    V_embed_bert1,\n",
    "    V_embed_bert2,\n",
    "    V_embed_bert3,\n",
    "    V_embed_bert4,\n",
    "], axis=1)\n",
    "\n",
    "del V_embed_bert1,V_embed_bert2,V_embed_bert3,V_embed_bert4\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#V_embed_concat.shape\n",
    "V_embed_concat = V_embed_concat / cupy.linalg.norm(V_embed_concat, ord=2, axis=1, keepdims=True) # l2 正则化\n",
    "\n",
    "print(V_embed_concat.shape)\n",
    "#del dataset, loader, model_bert_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603010b",
   "metadata": {
    "papermill": {
     "duration": 0.035856,
     "end_time": "2022-07-05T12:05:31.226429",
     "exception": false,
     "start_time": "2022-07-05T12:05:31.190573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414dd5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:05:31.622447Z",
     "iopub.status.busy": "2022-07-05T12:05:31.621382Z",
     "iopub.status.idle": "2022-07-05T12:13:11.918788Z",
     "shell.execute_reply": "2022-07-05T12:13:11.917815Z"
    },
    "papermill": {
     "duration": 460.337605,
     "end_time": "2022-07-05T12:13:11.918947",
     "exception": false,
     "start_time": "2022-07-05T12:05:31.581342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create candidate index country by country\n",
    "import faiss # 导入faiss\n",
    "\n",
    "def gen_candidate_ranks(df, V_embed, n_candidates):\n",
    "    '''\n",
    "    Generate candidate ranks for each row in df.\n",
    "    '''\n",
    "    res = faiss.StandardGpuResources() # GPU资源\n",
    "    index = faiss.IndexFlatIP(V_embed.shape[1]) # 创建索引\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index) # 将索引放到GPU上\n",
    "    index.add(V_embed) # 将embedding加入索引\n",
    "    D, I = index.search(V_embed, n_candidates) # 搜索最近的n_candidates个索引\n",
    "    D = np.clip(D, 0, 1) # 将D值限制在0和1之间\n",
    "    return D, I # 返回D和I\n",
    "\n",
    "\n",
    "# 寻找每个样本的n个最近邻值的距离和索引\n",
    "D_concat, I_concat = gen_candidate_ranks(df, cupy.asnumpy(V_embed_concat), CFG.n_candidates) \n",
    "np.save(\"D_concat.npy\", D_concat) # 保存D_concat\n",
    "np.save(\"I_concat.npy\", I_concat) # 保存I_concat\n",
    "\n",
    "# D_concat.shape == (1138812, 50) # 第一个值是和自己的距离\n",
    "# array([[0.99999994, 0.83693177, 0.34076163, ..., 0.22953738, 0.22911589,\n",
    "#         0.22874717],\n",
    "#        [1.        , 0.80496347, 0.41403773, ..., 0.23809814, 0.2379353 ,\n",
    "#         0.23778985],\n",
    "#        [0.99999934, 0.45569178, 0.35417274, ..., 0.18390407, 0.18360165,\n",
    "#         0.18332995],\n",
    "#        ...,\n",
    "#        [0.9999999 , 0.98086345, 0.9773909 , ..., 0.9356541 , 0.93557185,\n",
    "#         0.9349697 ],\n",
    "#        [0.99999917, 0.6578855 , 0.3543492 , ..., 0.23836766, 0.23802386,\n",
    "#         0.23755664],\n",
    "#        [1.        , 0.99317944, 0.47047997, ..., 0.22384529, 0.22381721,\n",
    "#         0.22305818]], dtype=float32)\n",
    "\n",
    "# I_concat.shape == (1138812, 50) # 第一个值是和自己的索引\n",
    "# array([[      0,  972683,  386676, ...,  199642,  655271,  925360],\n",
    "#        [      1, 1032766,  665938, ...,   94111,   70948,   68941],\n",
    "#        [      2,  657609,   85624, ...,  379627,  980143,  832934],\n",
    "#        ...,\n",
    "#        [1138809,  269067,  177162, ...,  614297,  381139,  974775],\n",
    "#        [1138810,   20470,  126901, ...,  357689,  135823,  794488],\n",
    "#        [1138811,  119981,  670304, ...,  201677,  931374,  137607]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fb766",
   "metadata": {
    "papermill": {
     "duration": 0.049211,
     "end_time": "2022-07-05T12:13:12.125095",
     "exception": false,
     "start_time": "2022-07-05T12:13:12.075884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DBA/QE weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a9b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:13:14.727652Z",
     "iopub.status.busy": "2022-07-05T12:13:14.726570Z",
     "iopub.status.idle": "2022-07-05T12:20:17.259195Z",
     "shell.execute_reply": "2022-07-05T12:20:17.258716Z"
    },
    "papermill": {
     "duration": 422.593759,
     "end_time": "2022-07-05T12:20:17.259341",
     "exception": false,
     "start_time": "2022-07-05T12:13:14.665582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/lyakaap/2nd-place-solution/notebook\n",
    "def query_expansion(V, D, I, alpha=3, k=2):\n",
    "    '''\n",
    "    对V_embed_concat进行查询扩展. \n",
    "    当前样本的embedding, 用最邻近的2个embedding乘以距离权重, 然后求和.\n",
    "\n",
    "    V: embedding matrix\n",
    "    D: distance matrix\n",
    "    I: index matrix\n",
    "    alpha: 次方系数\n",
    "    k: Top-k 邻近值\n",
    "    '''\n",
    "    weights = cupy.array(np.expand_dims(D[:, :k] ** alpha, axis=-1).astype(np.float32)) # 每个样本前k个邻近距离的3次方， shape==(1138812, 2, 1) \n",
    "    chunk_size = 100_000 # chunk大小, 防止OOM\n",
    "    for i in range(0, len(df), chunk_size): \n",
    "        # V.shape: (1138812, 960)\n",
    "        # I[i:i+chunk_size, :k].shape: (100000, 2) # 纯作为索引\n",
    "        # V[I[i:i+chunk_size, :k]].shape: (100000, 2, 960)\n",
    "        # weights[i:i+chunk_size].shape: (100000, 2, 1)\n",
    "        V[i:i+chunk_size] = (V[I[i:i+chunk_size, :k]] * weights[i:i+chunk_size]).sum(axis=1) # 将V中的2个embedding乘上，以距离为权重，然后求和\n",
    "    return V\n",
    "\n",
    "\n",
    "V_embed_concat = query_expansion(V_embed_concat, D_concat, I_concat) # 对V_embed_concat进行查询扩展. 当前样本的embedding, 用最邻近的2个embedding乘以距离权重, 然后求和.\n",
    "V_embed_concat /= np.linalg.norm(V_embed_concat, 2, axis=1, keepdims=True) # l2 正则化\n",
    "\n",
    "del D_concat,I_concat # 删除变量\n",
    "gc.collect() # 清空缓存\n",
    "torch.cuda.empty_cache() # 清空显存\n",
    "\n",
    "D_concat, I_concat = gen_candidate_ranks(df, cupy.asnumpy(V_embed_concat), CFG.n_candidates) # 对V_embed_concat进行索引\n",
    "np.save(\"D_concat_dbaqe.npy\", D_concat) # 保存D_concat\n",
    "np.save(\"I_concat_dbaqe.npy\", I_concat) # 保存I_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fff02",
   "metadata": {
    "papermill": {
     "duration": 0.064464,
     "end_time": "2022-07-05T12:20:17.389276",
     "exception": false,
     "start_time": "2022-07-05T12:20:17.324812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spatial candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8b97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:20:17.537725Z",
     "iopub.status.busy": "2022-07-05T12:20:17.536939Z",
     "iopub.status.idle": "2022-07-05T12:25:10.186958Z",
     "shell.execute_reply": "2022-07-05T12:25:10.187528Z"
    },
    "papermill": {
     "duration": 292.733068,
     "end_time": "2022-07-05T12:25:10.187690",
     "exception": false,
     "start_time": "2022-07-05T12:20:17.454622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "def gen_candidate_rank_spatial(df, n_candidates):\n",
    "    '''\n",
    "    对每个country内的样本进行k邻近查找\n",
    "    按经度/纬度的haversine距离查找candidates。\n",
    "    Returns:\n",
    "        I (np.array): I[s_index][r] -> s_index 的第r个最近的点的索引。\n",
    "    '''\n",
    "    for column in df[[\"latitude\", \"longitude\"]]:\n",
    "        rad = np.deg2rad(df[column].values) # 将经度或纬度 --> 弧度\n",
    "        df[f'{column}_rad'] = rad # 弧度 存入df\n",
    "\n",
    "    I = np.full((len(df), n_candidates), -1, dtype=np.int32) # 创建I矩阵, shape=(sample_number, n_candidates), 全为-1\n",
    "    for country, country_df in tqdm(df[[\"country\", \"latitude_rad\", \"longitude_rad\"]].to_pandas().groupby(\"country\")): # 对每个国家进行遍历\n",
    "        clip_n_candidates = min(len(country_df), n_candidates) # 最大只允许n_candidates个样本\n",
    "        country_df = country_df.reset_index() # 重置索引\n",
    "        ball = BallTree(country_df[[\"latitude_rad\", \"longitude_rad\"]].values, metric='haversine') # 创建BallTree\n",
    "        # 计算country_df中每个点到其他点的距离\n",
    "        _, indices = ball.query( \n",
    "            country_df[[\"latitude_rad\", \"longitude_rad\"]].values,  # 待查询点的经纬度\n",
    "            k = clip_n_candidates # k个最邻近点\n",
    "        )\n",
    "        \n",
    "        indices = np.concatenate(\n",
    "            [\n",
    "                indices,  # BallTree查询结果索引\n",
    "                np.zeros((len(indices), n_candidates - clip_n_candidates), dtype=np.int32) # 当candidates列数过少时，用0补全至n_candidates列数\n",
    "            ], axis=1\n",
    "        )\n",
    "        for i in range(n_candidates):\n",
    "            I[country_df[\"index\"].values, i] = country_df.loc[indices[:, i], \"index\"].values\n",
    "\n",
    "    return I\n",
    "\n",
    "\n",
    "I_spatial = gen_candidate_rank_spatial(df, 10) # 对每个country内的样本进行k邻近查找 shpe==(1138812, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bc002",
   "metadata": {
    "papermill": {
     "duration": 0.084071,
     "end_time": "2022-07-05T12:25:10.356945",
     "exception": false,
     "start_time": "2022-07-05T12:25:10.272874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tfdfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddc7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:10.536152Z",
     "iopub.status.busy": "2022-07-05T12:25:10.534720Z",
     "iopub.status.idle": "2022-07-05T12:25:19.323268Z",
     "shell.execute_reply": "2022-07-05T12:25:19.322336Z"
    },
    "papermill": {
     "duration": 8.881381,
     "end_time": "2022-07-05T12:25:19.323400",
     "exception": false,
     "start_time": "2022-07-05T12:25:10.442019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')  # 创建tfidf\n",
    "V_name = tfidf.fit_transform(df[\"name\"])  # 对name进行tfidf\n",
    "V_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efa363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:19.499237Z",
     "iopub.status.busy": "2022-07-05T12:25:19.498362Z",
     "iopub.status.idle": "2022-07-05T12:25:20.249235Z",
     "shell.execute_reply": "2022-07-05T12:25:20.249878Z"
    },
    "papermill": {
     "duration": 0.841569,
     "end_time": "2022-07-05T12:25:20.250099",
     "exception": false,
     "start_time": "2022-07-05T12:25:19.408530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english') # 创建tfidf\n",
    "df[\"full_address\"] = df[\"address\"] + \", \" + df[\"city\"] + \", \" + df[\"state\"] + \", \"  + df[\"country\"] # 创建full_address列, 为 address, city, state, country\n",
    "V_full_address = tfidf.fit_transform(df[\"full_address\"]) # 对full_address进行tfidf\n",
    "V_full_address.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e39f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:20.424398Z",
     "iopub.status.busy": "2022-07-05T12:25:20.423884Z",
     "iopub.status.idle": "2022-07-05T12:25:20.995528Z",
     "shell.execute_reply": "2022-07-05T12:25:20.994544Z"
    },
    "papermill": {
     "duration": 0.660662,
     "end_time": "2022-07-05T12:25:20.995679",
     "exception": false,
     "start_time": "2022-07-05T12:25:20.335017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english') # 创建tfidf\n",
    "V_cat = tfidf.fit_transform(df[\"categories\"].fillna(\"nocategory\")) # 对categories进行tfidf\n",
    "V_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb519ea0",
   "metadata": {
    "papermill": {
     "duration": 0.085516,
     "end_time": "2022-07-05T12:25:21.169037",
     "exception": false,
     "start_time": "2022-07-05T12:25:21.083521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc17e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:21.377953Z",
     "iopub.status.busy": "2022-07-05T12:25:21.376902Z",
     "iopub.status.idle": "2022-07-05T12:25:21.378704Z",
     "shell.execute_reply": "2022-07-05T12:25:21.379140Z"
    },
    "papermill": {
     "duration": 0.126215,
     "end_time": "2022-07-05T12:25:21.379273",
     "exception": false,
     "start_time": "2022-07-05T12:25:21.253058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = 6371.0  # 地球半径, 单位km\n",
    "\n",
    "def manhattan(lat1, long1, lat2, long2):\n",
    "    '''\n",
    "    计算曼哈顿距离\n",
    "    '''\n",
    "    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n",
    "\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/justfor/speedup-haversine/script\n",
    "    计算地球上两点之间的大圆距离的距离（以小数点后的度数指定）。\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1 # 经度差\n",
    "    dlat = lat2 - lat1 # 纬度差\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return km\n",
    "\n",
    "\n",
    "def create_features(df, i, indices):\n",
    "    '''\n",
    "    创建特征\n",
    "    '''\n",
    "    prev_i = max(i-1, 0) # i-1,最小值为0\n",
    "    next_i = min(i+1, indices.shape[1] - 1) # i+1,最大值为indices长度\n",
    "    \n",
    "    prev_cand_index = indices[:, prev_i] # 第i-1个相似度的candidate的索引\n",
    "    next_cand_index = indices[:, next_i] # 第i+1个相似度的candidate的索引\n",
    "    cand_index = indices[:, i] # 第i个相似度的candidate索引\n",
    "    \n",
    "    lon1 = df[\"longitude\"].to_pandas().to_numpy() # longitude列的值\n",
    "    lat1 = df[\"latitude\"].to_pandas().to_numpy() # latitude列的值\n",
    "    lon2 = df[\"longitude\"][cand_index].to_pandas().to_numpy() # 第i个相似度的candidate顺序的longitude列值\n",
    "    lat2 = df[\"latitude\"][cand_index].to_pandas().to_numpy() # 第i个相似度的candidate顺序的latitude列值\n",
    "    #df[\"diff_lon\"] = lon1 - lon2\n",
    "    #df[\"diff_lat\"] = lat1 - lat2\n",
    "    df[\"diff_lon\"] = np.abs(lon1 - lon2) # 经度差绝对值\n",
    "    df[\"diff_lat\"] = np.abs(lat1 - lat2) # 纬度差绝对值\n",
    "    \n",
    "    df[\"lonlat_eucdist\"] =  (df['diff_lon'] ** 2 + df['diff_lat'] ** 2) ** 0.5 # 经纬度差的平方根\n",
    "    df[\"lonlat_manhattan\"] = manhattan(lat1, lon1, lat2, lon2) # 曼哈顿距离\n",
    "    df[\"lonlat_haversine_dist\"] = haversine_np(lon1, lat1, lon2, lat2) # 地球距离\n",
    "    \n",
    "    df[\"name_cossim\"] = V_name.multiply(V_name[cand_index]).sum(axis=1).ravel() # 名称相似度\n",
    "    df[\"full_address_cossim\"] = V_full_address.multiply(V_full_address[cand_index]).sum(axis=1).ravel() # 地址相似度\n",
    "    df[\"cat_cossim\"] = V_cat.multiply(V_cat[cand_index]).sum(axis=1).ravel() # 类别相似度\n",
    "    \n",
    "    df[\"cand_hit_count_02\"] = df[\"hit_count_02\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.2的样本的candidate的概率\n",
    "    df[\"cand_hit_count_03\"] = df[\"hit_count_03\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.3的样本的candidate的概率\n",
    "    df[\"cand_hit_count_04\"] = df[\"hit_count_04\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.4的样本的candidate的概率 \n",
    "    df[\"cand_hit_count_05\"] = df[\"hit_count_05\"][cand_index].to_pandas().to_numpy()   # 预测概率>0.5的样本的candidate的概率\n",
    "    df[\"cand_hit_count_sum\"] = df[\"hit_count_sum\"][cand_index].to_pandas().to_numpy() # 所有样本的candidate的概率\n",
    "    \n",
    "    df[\"hit_count_02_min\"] = df[[\"hit_count_02\", \"cand_hit_count_02\"]].min(axis=1)  # 样本预测概率和其candidate的概率 的最小值（0.2）\n",
    "    df[\"hit_count_03_min\"] = df[[\"hit_count_03\", \"cand_hit_count_03\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.3）\n",
    "    df[\"hit_count_04_min\"] = df[[\"hit_count_04\", \"cand_hit_count_04\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.4）\n",
    "    df[\"hit_count_05_min\"] = df[[\"hit_count_05\", \"cand_hit_count_05\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（0.5）\n",
    "    df[\"hit_count_sum_min\"] = df[[\"hit_count_sum\", \"cand_hit_count_sum\"]].min(axis=1) # 样本预测概率和其candidate的概率 的最小值（所有样本）\n",
    "    \n",
    "    df[\"hit_count_02_max\"] = df[[\"hit_count_02\", \"cand_hit_count_02\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.2）\n",
    "    df[\"hit_count_03_max\"] = df[[\"hit_count_03\", \"cand_hit_count_03\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.3）\n",
    "    df[\"hit_count_04_max\"] = df[[\"hit_count_04\", \"cand_hit_count_04\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.4）\n",
    "    df[\"hit_count_05_max\"] = df[[\"hit_count_05\", \"cand_hit_count_05\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（0.5）\n",
    "    df[\"hit_count_sum_max\"] = df[[\"hit_count_sum\", \"cand_hit_count_sum\"]].max(axis=1) # 样本预测概率和其candidate的概率 的最大值（所有样本）\n",
    "    \n",
    "    cossim = []\n",
    "    eucdist = []\n",
    "    \n",
    "    eucdist1=[]\n",
    "    eucdist2=[]\n",
    "    eucdist3=[]\n",
    "    eucdist4=[]\n",
    "    eucdist5=[]\n",
    "    \n",
    "    chunk_size = 100_000 # 每次处理的数据量\n",
    "    for i in range(0, len(df), chunk_size):  # 以chunk取数据，防止OOM \n",
    "        cossim.append(cupy.multiply(V_embed_concat[i:i+chunk_size], V_embed_concat[cand_index[i:i+chunk_size]]).sum(axis=1)) # V_embed_concat 相似度\n",
    "        eucdist.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # V_embed_concat 欧式距离 cur - cand_index\n",
    "        \n",
    "        eucdist1.append(cupy.sqrt(((V_embed_concat[prev_cand_index[i:i+chunk_size]] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离1 prev_cand_index - cand_index\n",
    "        eucdist2.append(cupy.sqrt(((V_embed_concat[next_cand_index[i:i+chunk_size]] - V_embed_concat[cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离2 next_cand_index - cand_index\n",
    "        eucdist3.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size]                  - V_embed_concat[prev_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离3 cur - prev_cand_index\n",
    "        eucdist4.append(cupy.sqrt(((V_embed_concat[i:i+chunk_size]                  - V_embed_concat[next_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离4 cur - next_cand_index\n",
    "        eucdist5.append(cupy.sqrt(((V_embed_concat[prev_cand_index[i:i+chunk_size]] - V_embed_concat[next_cand_index[i:i+chunk_size]]) ** 2).sum(axis=1))) # 欧式距离5 prev_cand_index - next_cand_index\n",
    "    \n",
    "    # 将cupy数据转换为pandas数据\n",
    "    df[\"embed_cossim\"] = cupy.concatenate(cossim) \n",
    "    df[\"embed_eucdist\"] = cupy.concatenate(eucdist)\n",
    "    df[\"embed_eucdist1\"] = cupy.concatenate(eucdist1)\n",
    "    df[\"embed_eucdist2\"] = cupy.concatenate(eucdist2)\n",
    "    df[\"embed_eucdist3\"] = cupy.concatenate(eucdist3)\n",
    "    df[\"embed_eucdist4\"] = cupy.concatenate(eucdist4)\n",
    "    df[\"embed_eucdist5\"] = cupy.concatenate(eucdist5)\n",
    "    \n",
    "    # 欧式距离差\n",
    "    df[\"d0_d1\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist1\"] \n",
    "    df[\"d0_d2\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist2\"]\n",
    "    df[\"d0_d3\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist3\"]\n",
    "    df[\"d0_d4\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist4\"]\n",
    "    df[\"d0_d5\"] = df[\"embed_eucdist\"] - df[\"embed_eucdist5\"]\n",
    "    \n",
    "    for col in [\"id\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\", \"full_address\"]:\n",
    "        df[f\"{col}_edit_dist\"] = df[col].str.edit_distance(df[col][cand_index]) # Levenshtein 距离\n",
    "        # 标准化 Levenshtein 距离\n",
    "        df[f\"norm_{col}_edit_dist\"] = df[f\"{col}_edit_dist\"] / df[col].str.len() # Levenshtein 距离/长度\n",
    "        df[f\"norm_{col}_edit_dist\"] = df[f\"norm_{col}_edit_dist\"].replace([np.inf, -np.inf], 0) # 将inf替换为0\n",
    "    \n",
    "    features = [\n",
    "        \"diff_lon\",\n",
    "        \"diff_lat\",\n",
    "        \"lonlat_eucdist\",\n",
    "        \"lonlat_manhattan\",\n",
    "        \"lonlat_haversine_dist\",\n",
    "        \"name_cossim\",\n",
    "        \"full_address_cossim\",\n",
    "        \"cat_cossim\",\n",
    "        \"embed_cossim\",\n",
    "        \"embed_eucdist\",\n",
    "        \n",
    "        \"embed_eucdist1\",\n",
    "        \"embed_eucdist2\",\n",
    "        \"embed_eucdist3\",\n",
    "        \"embed_eucdist4\",\n",
    "        \"embed_eucdist5\",\n",
    "        \"d0_d1\",\n",
    "        \"d0_d2\",\n",
    "        \"d0_d3\",\n",
    "        \"d0_d4\",\n",
    "        \"d0_d5\",\n",
    "\n",
    "        \"hit_count_02\",\n",
    "        \"hit_count_03\",\n",
    "        \"hit_count_04\",\n",
    "        \"hit_count_05\",\n",
    "        \"hit_count_sum\",\n",
    "        \n",
    "        \"hit_count_02_min\",\n",
    "        \"hit_count_03_min\",\n",
    "        \"hit_count_04_min\",\n",
    "        \"hit_count_05_min\",\n",
    "        \"hit_count_sum_min\",\n",
    "        \"hit_count_02_max\",\n",
    "        \"hit_count_03_max\",\n",
    "        \"hit_count_04_max\",\n",
    "        \"hit_count_05_max\",\n",
    "        \"hit_count_sum_max\"\n",
    "    ]\n",
    "    \n",
    "    for col in [\"id\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"country\", \"url\", \"phone\", \"categories\", \"full_address\"]:\n",
    "        features.append(f\"{col}_edit_dist\") # 存下edit_dist类特征\n",
    "        features.append(f\"norm_{col}_edit_dist\") # 存下标准化后的edit_dist类特征\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82eff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:21.569142Z",
     "iopub.status.busy": "2022-07-05T12:25:21.568225Z",
     "iopub.status.idle": "2022-07-05T12:25:44.556352Z",
     "shell.execute_reply": "2022-07-05T12:25:44.555559Z"
    },
    "papermill": {
     "duration": 23.081925,
     "end_time": "2022-07-05T12:25:44.556528",
     "exception": false,
     "start_time": "2022-07-05T12:25:21.474603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    '''id与poi的映射'''\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    poi与id的映射\n",
    "    poi对应n个id\n",
    "    '''\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "id2poi = get_id2poi(df.to_pandas()) # id与poi的映射\n",
    "poi2ids = get_poi2ids(df.to_pandas()) # poi与id的映射\n",
    "\n",
    "\n",
    "def id2target_size(id_str: str):\n",
    "    return len(poi2ids[id2poi[id_str]]) # 返回id对应的poi数量\n",
    "\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    '''\n",
    "    计算得分\n",
    "    '''\n",
    "    scores = []\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()): # 循环每个id和对应的match_ids\n",
    "        targets = poi2ids[id2poi[id_str]] # 获取id对应的poi\n",
    "        preds = set(matches.split()) # 获取预测的poi\n",
    "        score = len((targets & preds)) / len((targets | preds)) # 计算得分\n",
    "        scores.append(score) # 存下得分\n",
    "    scores = np.array(scores) # 转换为numpy数组\n",
    "    return scores.mean() # 返回平均得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f85a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:44.732552Z",
     "iopub.status.busy": "2022-07-05T12:25:44.731783Z",
     "iopub.status.idle": "2022-07-05T12:25:44.734327Z",
     "shell.execute_reply": "2022-07-05T12:25:44.733820Z"
    },
    "papermill": {
     "duration": 0.0917,
     "end_time": "2022-07-05T12:25:44.734434",
     "exception": false,
     "start_time": "2022-07-05T12:25:44.642734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "THRESH = 0.4 # 阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39620759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:44.963269Z",
     "iopub.status.busy": "2022-07-05T12:25:44.962379Z",
     "iopub.status.idle": "2022-07-05T12:25:44.964892Z",
     "shell.execute_reply": "2022-07-05T12:25:44.964363Z"
    },
    "papermill": {
     "duration": 0.14604,
     "end_time": "2022-07-05T12:25:44.965003",
     "exception": false,
     "start_time": "2022-07-05T12:25:44.818963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train(I, n_candidates, params, cand_type):\n",
    "    '''\n",
    "    训练模型\n",
    "    '''\n",
    "    link = defaultdict(lambda:defaultdict(lambda: 10**10)) # 临时字典\n",
    "    \n",
    "    models = {}\n",
    "    kf = GroupKFold(n_splits=CFG.n_splits) # 分割训练集/验证集\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df.to_pandas(), df[\"point_of_interest\"].to_pandas(), df[\"point_of_interest\"].to_pandas())):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "    folds = df[\"fold\"].to_pandas().to_numpy()\n",
    "\n",
    "    # 初始化预测结果\n",
    "    df[\"hit_count_02\"] = 0\n",
    "    df[\"hit_count_03\"] = 0\n",
    "    df[\"hit_count_04\"] = 0\n",
    "    df[\"hit_count_05\"] = 0\n",
    "    df[\"hit_count_sum\"] = 0\n",
    "    \n",
    "    df[\"cand_hit_count_02\"] = 0\n",
    "    df[\"cand_hit_count_03\"] = 0\n",
    "    df[\"cand_hit_count_04\"] = 0\n",
    "    df[\"cand_hit_count_05\"] = 0\n",
    "    df[\"cand_hit_count_sum\"] = 0\n",
    "    \n",
    "    oof = []\n",
    "    # training\n",
    "    for i in range(I.shape[1]):\n",
    "        tmp_df = df.copy() # 复制一份df\n",
    "        tmp_df[\"target\"] = tmp_df[\"point_of_interest\"] == tmp_df[\"point_of_interest\"].to_pandas().values[I[:, i]] # 第i个相似度的candidate的target为True, 其他为False\n",
    "        \n",
    "        tmp_df[\"match_id\"] = tmp_df[\"id\"].to_pandas().values[I[:, i]] # 第i个相似度的candidate的id\n",
    "        tmp_df[\"id_target_size\"] = tmp_df.id.to_pandas().apply(id2target_size) # 与id相同poi的数量\n",
    "        tmp_df[\"match_id_target_size\"] = tmp_df.match_id.to_pandas().apply(id2target_size) # 与该match_id相同poi的数量\n",
    "        tmp_df[\"sample_weight\"] = (1 / tmp_df.id_target_size) + (1 / tmp_df.match_id_target_size) # 样本权重\n",
    "\n",
    "        tmp_df, features = create_features(tmp_df, i, I) # 创建特征\n",
    "\n",
    "        print(f\"Candidate rank {i} target mean = {tmp_df['target'].mean()}\") # 打印 相似度排名第i的target平均值\n",
    "\n",
    "        for fold in range(CFG.n_splits):\n",
    "            print(f\"== fold {fold} ==\")\n",
    "            train_idx = folds != fold # 训练集idx\n",
    "            test_idx = folds == fold # 测试集idx\n",
    "\n",
    "            X_train, y_train, train_weights = (\n",
    "                tmp_df.loc[train_idx, features], # 训练集特征\n",
    "                tmp_df.loc[train_idx, \"target\"].astype(int), # 训练集target\n",
    "                tmp_df.loc[train_idx, \"sample_weight\"], # 训练集权重\n",
    "            )\n",
    "            X_test, y_test = (\n",
    "                tmp_df.loc[test_idx, features], # 测试集特征\n",
    "                tmp_df.loc[test_idx, \"target\"].astype(int), # 测试集target\n",
    "            )\n",
    "            X_all, y_all = (\n",
    "                tmp_df[features], # 所有特征\n",
    "                tmp_df[\"target\"].astype(int) # 所有target\n",
    "            )\n",
    "\n",
    "            _oof = tmp_df.loc[test_idx, [\"id\", \"point_of_interest\", \"match_id\"]].to_pandas() # 测试集的id, point_of_interest, match_id\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, y_train, weight=train_weights) # 训练集dmatrix\n",
    "            dtest = xgb.DMatrix(X_test, y_test) # 测试集dmatrix\n",
    "            dall = xgb.DMatrix(X_all, y_all) # 所有数据dmatrix\n",
    "\n",
    "            xgb_model = xgb.train(\n",
    "                params=params, # 参数\n",
    "                dtrain=dtrain, # 训练集dmatrix\n",
    "                num_boost_round=5_000, # 迭代次数\n",
    "                evals=[(dtrain, \"train\"), (dtest, \"test\")], # 评估集\n",
    "                early_stopping_rounds=100, # 早停\n",
    "                verbose_eval=500, # 每500次迭代打印一次\n",
    "            )\n",
    "            xgb_model.save_model(f\"fs_xgb_model_{cand_type}_candidate{i}_fold{fold}.json\") # 保存模型\n",
    "            models[i] = xgb_model # 保存模型\n",
    "            _oof[\"pred\"] = xgb_model.predict(dtest, ntree_limit=xgb_model.best_iteration) # 预测\n",
    "\n",
    "            oof.append(_oof.query(\"pred >= @THRESH\")) # 预测结果（大于某个阈值）\n",
    "            all_pred = xgb_model.predict(dall, ntree_limit=xgb_model.best_iteration) # 预测所有数据\n",
    "            \n",
    "            ids = _oof.id.values # 测试集id\n",
    "            match_ids = _oof.match_id.values # 测试集match_id\n",
    "            pred_lis = _oof.pred.values # 测试集预测结果\n",
    "            for i in range(len(_oof)):\n",
    "                dist = 1 - pred_lis[i] # 距离 = 1-预测概率\n",
    "\n",
    "                if dist > 0.9: # 如果距离大于0.9，则跳过\n",
    "                    continue\n",
    "                current_min = min([link[ids[i]][match_ids[i]], link[match_ids[i]][ids[i]], dist]) # 更新最小距离\n",
    "                # 保存最小id和match_id的距离\n",
    "                link[match_ids[i]][ids[i]] = current_min \n",
    "                link[ids[i]][match_ids[i]] = current_min \n",
    "\n",
    "        #hit_count \n",
    "        df[\"hit_count_02\"] += (all_pred > 0.2) # 预测结果大于0.2\n",
    "        df[\"hit_count_03\"] += (all_pred > 0.3) # 预测结果大于0.3\n",
    "        df[\"hit_count_04\"] += (all_pred > 0.4) # 预测结果大于0.4 \n",
    "        df[\"hit_count_05\"] += (all_pred > 0.5) # 预测结果大于0.5\n",
    "        df[\"hit_count_sum\"] += all_pred # 预测结果\n",
    "        \n",
    "        gc.collect() # 清空内存\n",
    "        torch.cuda.empty_cache() # 清空cuda缓存\n",
    "\n",
    "    oof = pd.concat(oof, axis=0).reset_index(drop=True) # 合并测试集的预测结果\n",
    "    print(\"Done training.\")\n",
    "\n",
    "    return models, oof, link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f26268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:25:45.762545Z",
     "iopub.status.busy": "2022-07-05T12:25:45.760888Z",
     "iopub.status.idle": "2022-07-05T12:51:09.819895Z",
     "shell.execute_reply": "2022-07-05T12:51:09.819321Z"
    },
    "papermill": {
     "duration": 1524.767805,
     "end_time": "2022-07-05T12:51:09.820032",
     "exception": false,
     "start_time": "2022-07-05T12:25:45.052227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect() # 清空内存\n",
    "torch.cuda.empty_cache() # 清空cuda缓存\n",
    "\n",
    "models_embed, oof_embed, link_embed = train(I_concat, CFG.n_candidates, params={\n",
    "    \"objective\": \"binary:logistic\", # 二分类\n",
    "    \"tree_method\": \"gpu_hist\", # GPU搜索\n",
    "    \"verbosity\": 0, # 不打印日志\n",
    "    \"learning_rate\": 0.05, # 学习率\n",
    "    \"max_depth\": 8, # 树最大深度\n",
    "    \"min_child_weight\": 0, # 叶子节点最小权重\n",
    "    \"lambda\": 1.0, # L2正则化\n",
    "    \"alpha\": 0.1, # L1正则化\n",
    "}, cand_type=\"embed\") # 训练bert embedding特征\n",
    "\n",
    "models_spatial, oof_spatial, link_spatial = train(I_spatial, CFG.n_candidates, params={\n",
    "    \"objective\": \"binary:logistic\", # 二分类\n",
    "    \"tree_method\": \"gpu_hist\", # GPU搜索\n",
    "    \"verbosity\": 0, # 不打印日志\n",
    "    \"learning_rate\": 0.05, # 学习率\n",
    "    \"max_depth\": 8, # 树最大深度 \n",
    "    \"min_child_weight\": 0, # 叶子节点最小权重\n",
    "    \"lambda\": 1.0,  # L2正则化\n",
    "    \"alpha\": 0.1,   # L1正则化\n",
    "}, cand_type=\"spatial\") # 训练spatial特征\n",
    "\n",
    "oof = pd.concat([oof_embed, oof_spatial], axis=0) # 合并两个模型的预测结果\n",
    "\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab72afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T12:51:10.190180Z",
     "iopub.status.busy": "2022-07-05T12:51:10.160433Z",
     "iopub.status.idle": "2022-07-05T12:51:24.112209Z",
     "shell.execute_reply": "2022-07-05T12:51:24.112911Z"
    },
    "papermill": {
     "duration": 14.124158,
     "end_time": "2022-07-05T12:51:24.113106",
     "exception": false,
     "start_time": "2022-07-05T12:51:09.988948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = oof_embed.groupby(\"id\")[\"match_id\"].apply(list).reset_index() \n",
    "tmp[\"matches\"] = tmp[\"match_id\"].apply(lambda x: \" \".join(set(x)))\n",
    "print(f\"Score {get_score(tmp)}, THRESH {THRESH}\") # embedding 分数 0.997\n",
    "\n",
    "tmp = oof_spatial.groupby(\"id\")[\"match_id\"].apply(list).reset_index()\n",
    "tmp[\"matches\"] = tmp[\"match_id\"].apply(lambda x: \" \".join(set(x)))\n",
    "print(f\"Score {get_score(tmp)}, THRESH {THRESH}\") # spatial 分数 0.877\n",
    "\n",
    "tmp = oof.groupby(\"id\")[\"match_id\"].apply(list).reset_index() # \n",
    "tmp[\"matches\"] = tmp[\"match_id\"].apply(lambda x: \" \".join(set(x)))\n",
    "print(f\"Score {get_score(tmp)}, THRESH {THRESH}\") # 合并两种特征的分数 0.996"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2909.53454,
   "end_time": "2022-07-05T12:51:43.347518",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-05T12:03:13.812978",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
